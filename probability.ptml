<html>
    <head>
        <link href="style.css" rel="stylesheet" type="text/css"/>
        <title>
            Discrete Probability
        </title>
    </head>

    <body>

<!--include menu.txt -->

        <h1 class="chap-title">
            Discrete Probability
        </h1>
            <div style="text-align:center">
                <figure class="lead-figure">
                    <img
                        src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/PascalTriangleAnimated2.gif/220px-PascalTriangleAnimated2.gif">
                </figure>
            </div>
            
            <details>
                <summary class="sum1">
                An Introduction to Discrete Probability 
                </summary>
                <details>
                    <summary class="sum2">
                        Finite Probability
                    </summary>
                    <p>
                        An experiment is a procedure that 
                        yields one of a given set of 
                        possible outcomes. The sample space of 
                        the experiment is the set of possible 
                        outcomes. An event is a subset 
                        of the sample space. 
                    </p>
                    <p class="def">
                        DEFINITION 1
                    </p>
                </details>
                <details>
                    <summary class="sum2">
                        Probabilities of Complements and Unions of Events
                    </summary>
                    <details>
                        <summary class="sum3">
                            THEOREM 1
                        </summary>
                    </details>
                    <details>
                        <summary class="sum3">
                            THEOREM 2
                        </summary>
                    </details>
                </details>

                <details>
                    <summary class="sum2">
                        Probabilistic Reasoning
                    </summary>
                </details>


                <details>
                    <summary class="sum2">
                        The Monty Hall Problem Video
                    </summary>

                    <figure>
                        <iframe width="560" height="315"
                            src="https://www.youtube.com/embed/4Lb-6rxZxx0"
                            frameborder="0" allowfullscreen>
                        </iframe>
                        <figcaption>
                        </figcaption>
                    </figure>
                </details>

            </details>

            <details>
                <summary class="sum1">
                Probability Theory 
                </summary>
                <details>
                    <summary class="sum2">
                        Assigning Probabilities
                    </summary>
                    <details>
                        <summary class="sum3">
                            DEFINITION 1
                        </summary>
                    </details>
                    <details>
                        <summary class="sum3">
                            DEFINITION 2
                        </summary>
                    </details>
                </details>
                <details>
                    <summary class="sum2">
                        Probabilities of Complements and 
                        Unions of Events
                    </summary>

                    <p>
                        Complements:
                        <br />
                        p(<span class="over"><i>E</i></span>)
                            = 1 - <i>p</i>(<i>E</i>)
                    </p>

                    <p class="def">
                            THEOREM 1
                    </p>
                    <p>
                        The probability of any event composed of a
                        sequence of disjoint events is just the sum of 
                        the individual probabilities.
                    </p>

                    <p class="def">
                            THEOREM 2
                    </p>
                    <p>
                        If <i>E</i> and <i>F</i> are events in the 
                        sample space <i>S</i>, then:
                        <br />
                        <i>p</i>(<i>E</i> &cup; <i>F</i>)
                        = <i>p</i>(<i>E</i>) + <i>p</i>(<i>F</i>)
                        &minus; <i>p</i>(<i>E</i> &cap; <i>F</i>)
                    </p>

                </details>
                <details>
                    <summary class="sum2">
                        Conditional Probability
                    </summary>
                    <p class="def">
                            DEFINITION 3
                    </p>

                    <p>
                     The <i>conditional probability</i> of
                     event <i>E</i> given event <i>F</i> has
                     occurred is defined as:
                    </p>

                    <figure>
                        <img src="graphics/CondProb.gif">
                        <figcaption>
                        </figcaption>
                    </figure>

                    <figure>
                        <img
                        src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/White_shark.jpg/240px-White_shark.jpg">
                        <figcaption>
                            When the shark bites, with his teeth big...
                        </figcaption>
                    </figure>
                    <p>
                        Let's analyze with our shark example:
                        <br />
                        <i>E</i> = "bitten by shark" (imagine it's .0005)
                        <br />
                        <i>F</i> = "shark present" (imagine it's .001)
                        <br />
                        And <i>E</i> &cap; <i>F</i> = .0005
                        <br />
                        Therefore, <i>p</i>(<i>E</i> | <i>F</i>) = .5
                        <br />
                        Or, "Half the time, if there is a shark around, it
                        bites someone."
                    </p>
                </details>
                <details>
                    <summary class="sum2">
                        Independence
                    </summary>
                    <p class="def">
                        DEFINITION 4
                    </p>

                    <p>
                        Events <i>E</i> and <i>F</i> are <i>independent</i>
                        if and only if <i>p</i>(<i>E</i> &cap; <i>F</i>) 
                        = <i>p</i>(<i>E</i>)<i>p</i>(<i>F</i>)
                    </p>

                    <p>
                        Notice how this was <i>not</i> the case with the shark
                        attacks! <i>Given</i> a shark is around, the
                        probability of a shark bite is much higher than
                        <i>p</i>(<i>E</i>)<i>p</i>(<i>F</i>) (which was .0000005).
                        This would not be
                        true if <i>F</i> were, say, "UFO present."
                    </p>

                    <p>
                        <b>Pairwise independence versus mutual independence:</b>
                        <br />
                        The best way to understand this difference is to
                        consider a situation like this, with two coin tosses:
                        <br />
                        Event <i>A</i> is both tosses are the same.
                        <br />
                        Event <i>B</i> is the first toss is heads.
                        <br />
                        Event <i>C</i> is the second toss is heads.
                        <br />
                        Notice that if <i>B</i> and <i>C</i> both occur, or 
                        both don't occur, <i>A</i> occured for sure.
                        And if only one of <i>B</i> and <i>C</i> occured, 
                        <i>A</i> didn't occur for sure.
                        <br />
                        Nevertheless, any two pairs of these events, taken on
                        their own, <i>are</i> independent.
                    </p>

                    <ul class="nested">
                        <li>
                        <a
                            href="https://en.wikipedia.org/wiki/Pairwise_independence#Example">
                            Example for pairwise independence versus mutual
                            independence
                        </a>
                        </li>
                        <li>
                            <a
                                href="https://www.khanacademy.org/math/precalculus/prob-comb/dependent-events-precalc/v/independent-events-1">
                                Khan Academy on independent events
                            </a>
                        </li>
                    </ul>

                </details>
                <details>
                    <summary class="sum2">
                        Bernoulli Trials and the Binomial Distribution
                    </summary>
                    <p>
                     <b>Bernoulli trial</b>: an experiment with only two
                     possible outcomes. (The patient lives, or the patient
                     dies.)
                     <br />
                     <i>p</i> + <i>q</i> = 1
                    </p>

                    <p>
                    Many problems can be solved by determining
                    the probability
                    of <i>k</i> successes when an
                    experiment consists of <i>n</i> mutually
                    independent Bernoulli trials.
                    (Bernoulli trials are
                    mutually independent if the conditional probability of
                    success on any given trial is <i>p</i>,
                    given any information
                    whatsoever about the outcomes of the other trials.) 
                    </p>

                    <p class="def">
                        THEOREM 2
                    </p>
                    <p>
                     The probability of exactly <i>k</i> successes in
                     <i>n</i> independent
                     Bernoulli trials, with probability of success <i>p</i> and
                     probability of failure <i>q</i> = 1
                     &minus; <i>p</i>, is

                     <i>C</i>(<i>n</i>, <i>k</i>)<i>p</i><sup><i>k</i></sup><i>q</i><sup><i>n</i>&minus;<i>k</i></sup>
                    </p>

                    <p>
                        <b>Example:</b> Suppose that the probability that a 0
                        bit is generated is 0.9, that the probability that a 1
                        bit is generated is 0.1, and that bits are generated
                        independently. What is the probability that exactly
                        eight 0 bits are generated when 10 bits are generated?
                        <br />
                        <b>Answer:</b>
                        <i>C</i>(10, 8)(0.9)<sup>8</sup>(0.1)<sup>2</sup>
                        = 0.1937102445
                    </p>

                </details>
                <details>
                    <summary class="sum2">
                        Random Variables
                    </summary>
                    <p class="def">
                        DEFINITION 6
                    </p>
                    <p>
                        A <i>random variable</i> is a function from
                        the sample space of an experiment to the set
                        of real numbers. A random variable assigns a
                        real number to each possible outcome.
                        <br />
                        <b>Example</b>: Let <i>X</i>(<i>t</i>) be the
                        number of heads that appear in three coin tosses.
                        Then, <i>X</i>(HHH) = 3, <i>X</i>(HHT) = 2,
                        and so on.
                    </p>

                    <p class="def">
                        DEFINITION 7
                    </p>
                    
                    <p>
                        The <i>distribution</i> of a random variable is the
                        complete set of probabilities for each possible 
                        value of the variable.
                        <br />
                        <b>Example</b>: In the above case of three coin tosses,
                        <i>P</i>(<i>X</i> = 3) = 1/8, <i>P</i>(<i>X</i> = 2) =
                        3/8, and so on.
                        <br />
                        We will study how to <i>use</i> random variables in
                        section 7.4.
                    </p>

                </details>
                <details>
                    <summary class="sum2">
                        The Birthday Problem
                    </summary>
                    <figure>
                        <img
                         src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Birthday_candles.jpg/250px-Birthday_candles.jpg">
                        <figcaption>
                        </figcaption>
                    </figure>
                    <p>
                        How many people have to enter a party before it is
                        likely (p &gt; .5) that two of them have the same 
                        birthday?
                        <br />
                        Note that this <i>resembles</i> a pigeonhole problem,
                        but with an important difference: we are not asking how
                        many do we need to be <i>certain</i> two have the same
                        birthday (answer: 367), but how many do we need for it
                        to be <i>likely</i>?
                    </p>
                    <p>
                        We are going to solve this by solving the complement
                        of the problem: we will solve for <i>p<sub>n</sub></i>,
                        the probability that they all have different birthdays,
                        and then the probability two will have the same
                        birthday will be 1 &minus; <i>p<sub>n</sub></i>.
                        The first person obviously will not have "the same"
                        birthday as anyone else. The probability the 
                        second person will have a different birthday is 
                        365 / 366. For the third (assuming no match yet!)
                        it is 364 / 366. And so on.
                    </p>
                    <p>
                        So our question becomes, at what point does 
                        1 &minus; <i>p<sub>n</sub></i> become greater 
                        than .5? And the answer is the surprisingly small
                        number 23!
                        <br />
                        All of this would be just a curiosity, except it
                        is the same question as "How likely is it our
                        hashing function will produce a collision after
                        hashing <i>n</i> items?"
                        <br />
                        In hashing, we are mapping <i>n</i> keys to
                        <i>m</i> slots, where <i>n</i> is usually
                        much greater than <i>m</i>. (For instance, possible
                        student names to 100 slots in a class roster.)
                        We want to know if students are randomly mapped
                        to a slot with probability 1/<i>m</i>, how likely
                        is it that two students will map to the same slot?
                        (Too many collisions slow down our hashing algorithm.)
                    </p>
                </details>

                <details>
                    <summary class="sum2">
                        Monte Carlo Algorithms
                    </summary>
                    <figure>
                        <img
                        src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Roulette_casino.JPG/1024px-Roulette_casino.JPG"
                         height="300" width="400">
                        <figcaption>
                        </figcaption>
                    </figure>
                    <p>
                        These are examples of <i>probabilistic 
                        algorithms</i>. Most of the time, we study 
                        algorithms that produce a definite answer.
                        But probabilistic algorithms produce correct
                        answers only with some likelihood.
                        A <i>Monte Carlo Algorithm</i> runs repeated 
                        trials of some process until the answer it gives is
                        "close enough" to certainty for the purposes of
                        the person or persons employing the algorithm.
                    </p>

                    <p>
                        <b>Example</b>: For an integer that is not prime, it
                        will pass Miller's test for fewer than <i>n</i>/4 bases
                        <i>b</i> with 1 &lt; <i>b</i> &lt; <i>n</i>. We can
                        generate a large number and try, say, <i>k</i>
                        iterations of the algorithm, and if it passes each 
                        iteration, the that it is not prime are
                        (1/4)<sup><i>k</i></sup>. For 30 iterations, this will
                        give us a probability that <i>n</i> is composite
                        of less than 1/10<sup>18</sup>.
                    </p>

                </details>
                <details>
                    <summary class="sum2">
                        The Probabilistic Method
                        <br />
                        Not Covered Fall 2017
                    </summary>

                    <p class="def">
                        THEOREM 3
                    </p>

                    <p class="def">
                        THEOREM 4
                    </p>

                </details>
            </details>

            <details>
                <summary class="sum1">
                Bayes' Theorem 
                <br />
                NOT COVERED FALL 2017
                </summary>
                <details>
                    <summary class="sum2">
                        THEOREM 1
                    </summary>
                </details>
                <details>
                    <summary class="sum2">
                        THEOREM 2
                    </summary>
                </details>
                <details>
                    <summary class="sum2">
                        Bayesian Spam Filter
                    </summary>
                </details>
            </details>

            <details>
                <summary class="sum1">
                Expected Value and Variance 
                </summary>
                <details>
                    <summary class="sum2">
                        Expected Values
                    </summary>
                   
                    <p class="def">
                        DEFINITION 1
                    </p>
                    
                    <p>
                        <i>s</i> being an event in <i>S</i>, then
                        <i>i</i>E(<i>X</i>) = &Sigma;<i>p</i>(<i>s</i>)
                        <i>X</i>(<i>s</i>)
                        <br />
                        This is also called the <i>mean</i> of the random
                        variable.
                        <br />
                        <b>Example:</b> The expected value of 1 toss of 
                        a fair die is 7/2.
                    </p>

              
                    <p class="def">
                        THEOREM 1
                    </p>
               
                    <p>
                        We can also get the above by summing up the
                        probability-weighted values for <i>X</i> being
                        equal to each outcome.
                    </p>

                
                    <p class="def">
                        THEOREM 2
                    </p>
                    <p>
                        The expected number of successes in <i>n</i>
                        Bernoulli trials where the probability of 
                        success in each trial is <i>p</i> = 
                            <i>np</i>.
                    </p>

                </details>
                <details>
                    <summary class="sum2">
                        Linearity of Expectations
                    </summary>   

                    <p class="def">
                        THEOREM 3
                    </p>

                </details>
                <details>
                    <summary class="sum2">
                        Average-Case Computational Complexity
                    </summary>
                </details>
                <details>
                    <summary class="sum2">
                        The Geometric Distribution
                    </summary>

                    <p class="def">
                        DEFINITION 2
                    </p>


                    <p class="def">
                        DEFINITION 3
                    </p>

                    <p class="def">
                        THEOREM 5
                    </p>

                </details>

                <details>
                    <summary class="sum2">
                        Independent Random Variables
                    </summary>
                    <p>
                    Two random variables <i>X</i> and <i>Y</i>
                    are independent if:
                    <i>p</i>(<i>X</i> = <i>r</i><sub>1</sub> and
                    <i>Y</i> = <i>r</i><sub>2</sub>) =
                    <i>p</i>(<i>X</i> = <i>r</i><sub>1</sub>) *
                    <i>p</i>(<i>Y</i> = <i>r</i><sub>2</sub>)
                    </p>

                </details>

                <details>
                    <summary class="sum2">
                        Variance
                    </summary>

                    <p class="def">
                        DEFINITION 4
                    </p>

                    <p class="def">
                        THEOREM 6
                    </p>

                    <p class="def">
                        COROLLARY 1
                    </p>

                    <p>
                        If <i>X</i> is a random variable,
                        <i>V</i>(<i>X</i>) = <i>E</i>((<i>X</i>
                        &minus; <i>&mu;</i>)<sup>2</sup>) 
                    </p>


                    <p class="def">
                        THEOREM 7
                    </p>

                    <details>
                        <summary class="sum3">
                            Chebyshev's Inequality
                        </summary>

                        <p class="def">
                            THEOREM 8
                        </p>

                    </details>
                </details>
            </details>

    </body>
</html>
